{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_images = convert_from_path(\"Input/set-1/SP_MIS02824100914340 1.pdf\")\n",
    "image = pdf_images[0].convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manually set the full path to tesseract.exe\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ocr_data)\n",
    "len(ocr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract words and their bounding boxes\n",
    "\n",
    "words = []\n",
    "boxes = []\n",
    "\n",
    "for i in range(len(ocr_data['text'])):\n",
    "    if ocr_data['text'][i].strip():\n",
    "        words.append(ocr_data['text'][i])\n",
    "        x, y, w, h = (ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i])\n",
    "        boxes.append([x, y, x+w, y+h])\n",
    "\n",
    "\n",
    "print('Extracted Words: ', words)\n",
    "print('Bounding Boxes: ', boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n",
    "len(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Bounding Boxes: [[896, 90, 1012, 112], [1023, 91, 1144, 113], [1156, 92, 1269, 113], [1278, 104, 1285, 106], [1296, 92, 1350, 113]]\n",
      "Normalized Bounding Boxes: [[527, 40, 595, 50], [601, 41, 672, 51], [680, 41, 746, 51], [751, 47, 755, 48], [762, 41, 794, 51]]\n"
     ]
    }
   ],
   "source": [
    "# Get image dimensions (width, height)\n",
    "image_width, image_height = image.size  # Assuming `image` is a PIL Image\n",
    "\n",
    "# Normalize bounding boxes to fit in [0, 1000] range\n",
    "normalized_boxes = [\n",
    "    [\n",
    "        int((x / image_width) * 1000),   # Normalize x1\n",
    "        int((y / image_height) * 1000),  # Normalize y1\n",
    "        int((x_w / image_width) * 1000), # Normalize x2\n",
    "        int((y_h / image_height) * 1000) # Normalize y2\n",
    "    ]\n",
    "    for (x, y, x_w, y_h) in boxes\n",
    "]\n",
    "\n",
    "# Debugging: Print some normalized values\n",
    "print(\"Original Bounding Boxes:\", boxes[:5])\n",
    "print(\"Normalized Bounding Boxes:\", normalized_boxes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\vuvee\\anaconda3\\envs\\dev\\Lib\\site-packages\\transformers\\modeling_utils.py:1072: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 16, 269, 474, 506]])\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained LayoutLMv3 processor and model\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=4)\n",
    "\n",
    "# Tokenization\n",
    "encoding = processor(images=image, text=words, boxes=normalized_boxes, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Run model inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "\n",
    "# Get predicted labels\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
