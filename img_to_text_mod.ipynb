{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image as PImage\n",
    "from transformers import DonutProcessor\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "pro_dir = \"last_trained_donut_pro\"\n",
    "mod_dir = \"last_trained_donut_mod\"\n",
    "con_dir = \"last_trained_donut_con\"\n",
    "\n",
    "os.environ['PYTORCH_HIP_ALLOC_CONF']='expandable_segments:True'\n",
    "path = \"Input/W2_Single_Clean_jpg/\"\n",
    "# set this variable as true if you want to perform your own sanity checks to make sure the data is correct\n",
    "sanity_check = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(path):\n",
    "    files = glob.glob(f\"{path}/*.jpg\")\n",
    "    loaded_images = []\n",
    "    for f in files:\n",
    "        img = PImage.open(f)\n",
    "        loaded_images.append(img)\n",
    "    return loaded_images\n",
    "\n",
    "imgs = load_images(path)\n",
    "if sanity_check:\n",
    "    imgs[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    1280,\n",
      "    960\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 10\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_length\": 30,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57525\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image = imgs[2]\n",
    "\n",
    "#After searching a bit, Donut seems to be the best OCR replacement. This version the finetuned-docvqa is ideal for more generalized 'find thing in pdf' usage. \n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(mod_dir)\n",
    "\n",
    "model.to(device)\n",
    "def question_result(model, question:str, image):\n",
    "    task_prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n",
    "\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    outputs = model.generate(\n",
    "\n",
    "        pixel_values.to(device),\n",
    "\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    " \n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "\n",
    "        use_cache=True,\n",
    "\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "\n",
    "        return_dict_in_generate=True,\n",
    "\n",
    "    )\n",
    "    end_token = \"</s_question><s_answer>\"\n",
    "    start_token = \"<s_docvqa><s_question>\"\n",
    "    string_fixer = len(start_token) + len(end_token) + len(question)\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    good_enough = sequence[string_fixer:]\n",
    "    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "    result = processor.token2json(sequence)\n",
    "    result = good_enough\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions_o_Concern = ['What is the social security number?', \n",
    "                       \"What is the Employer identification number?\", \n",
    "                       \"What is the Employee's first and last Name?\", \n",
    "                       \"What is the Employer's name?\", \n",
    "                       \"What is the Employee's zip code?\",\n",
    "                       \"What is the wages and tips?\", \n",
    "                       \"what is the address that begins with 365?\", \n",
    "                       \"What is the year of Wage and Tax Statement?\", \n",
    "                       \"What is the information being furnished to?\", \n",
    "                       \"Ignore watermarks, what is the employee's address?\", \n",
    "                       \"What is the zipcode?\", \"What's inside of box e?\", \n",
    "                       \"What is the state inside of box e?\", \n",
    "                       \"What is the employee's Address?\", \n",
    "                       \"What is the data inside of box f?\", \n",
    "                       \"What is the Employer's state ID number?\", \n",
    "                       \"What is the second Employer's state ID number?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JpegImageFile' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m Questions_o_Concern:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_result(model,\u001b[38;5;250m \u001b[39mq,\u001b[38;5;250m \u001b[39m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'JpegImageFile' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for q in Questions_o_Concern:\n",
    "    print(f\"{q} : {question_result(model, q, image)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_transform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
